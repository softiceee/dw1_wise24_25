{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0267bec-9318-4f49-8756-4681a76c765c",
   "metadata": {},
   "source": [
    "# Regressionsanalyse mit Random Forests\n",
    "\n",
    "Random Forests wurden Anfang der 1990er als Verbesserung zu einfachen Decision Trees vorgeschlagen. Die Idee dabei ist die Kombinierung vieler einzelner Decision Trees als Ensemble (Forest), um insgesamt eine bessere Vorhersagefähigkeit zu erhalten. Die einzelnen Decision Trees werden unabhängig voneinander trainiert und ihre Vorhersagen am Ende in Form eines finalen Votings (Durchschnittsberechnung bei Regression, Mehrheitsentscheid bei Klassifizierung) zusammengeführt. Daneben werden einige weitere Methoden genutzt, um insgesamt die Modellvorhersagefähigkeit zu steigern:\n",
    "\n",
    "* Bootstrap Sampling: Jeder Decision Tree erhält ein eigenes Sample aus den Trainingsdaten. Beim Bootstrap Sampling wird ein Anteil der Trainingsdaten zufällig ausgewählt und der Rest wird durch stellvertretende Datenpunkte (z. B. der Durchschnitt des ganzen Samples) aufgefüllt. Das führt dazu, dass die einzelnen Decision Trees über diverse und individuelle Training Sets verfügen. Das gesamte Ensemble wird dadurch robuster, dass die einzelnen Trees einzigartiger werden.\n",
    "* Feature Randomness: Bei jedem Split von jedem Decision Tree wird nur ein zufällig ausgewählter Teil der Prädiktoren des Datensatzes gesplittet. Auch hier gilt es, die einzelnen Decision Trees möglichst stark zu diversifizieren.\n",
    "* Growing Trees deep: Die einzelnen Decision Trees werden auf viele Splits (also auf eine höhere Komplexität) trainiert. Dadurch overfitten sie ihre jeweiligen Bootstrap Samples. Da die finale Vorhersage jedoch durch das gesamte Ensemble getroffen wird, wird das Overfitting wieder ausgeglichen.\n",
    "\n",
    "Das folgende Notebook hat keine Code-Aufgaben und ist optional für alle diejenigen, die Lust haben, ein weiteres Modell kennenzulernen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e73b8-9583-435f-baf4-e90c8d6fce44",
   "metadata": {},
   "source": [
    "## Daten vorbereiten\n",
    "\n",
    "Bei der Vorbereitung des Datensatzes gehen wir zunächst analog zum Decision Tree Modell vor. \n",
    "\n",
    "Wir müssen die Daten wieder laden, aufräumen und Variablen entfernen, die wir nicht als Prädiktoren nutzen wollen. Anschließend teilen wir die Daten in Trainings- und Testdaten auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1d825-34d0-427c-8c3f-53c80071c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacman::p_load(tidyverse, tidymodels, vip)\n",
    "set.seed(123)\n",
    "\n",
    "data <- read_csv(\"data/regression/bike_sharing.csv\", show_col_types = FALSE) %>%\n",
    "    mutate(across(where(is.character), as.factor)) %>%\n",
    "    janitor::clean_names() %>%\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c302a9-e04e-40e7-bf15-5d124cc033e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split <- data %>%\n",
    "    select(-instant, -dteday) %>%\n",
    "    select(-casual, -registered) %>%\n",
    "    initial_split(prop = 0.75)\n",
    "\n",
    "train_data <- training(data_split)\n",
    "test_data <- testing(data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b793e-f831-4852-bb45-04caf930317d",
   "metadata": {},
   "source": [
    "## Modell trainieren\n",
    "\n",
    "Wir können zunächst ein einfaches Random Forest-Modell trainieren. Random Forests sind in `parsnip` über die Funktion [`rand_forest()`](https://parsnip.tidymodels.org/reference/rand_forest.html) implementiert. Ähnlich wie bei `decision_tree()` stehen hier wieder eine Auswahl an Algorithmen zur Verfügung, die wir nutzen können. Der Default-Algorithmus ist die \"ranger\"-Engine. Wir definieren also wieder unser Modell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac9de5-fc71-4ce6-ac86-6acf45043c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_model <- rand_forest(\n",
    "    mode = \"regression\"            # Build a regression model\n",
    "    ) %>%\n",
    "    set_engine(\n",
    "        \"ranger\",                  # Using the `ranger` engine\n",
    "        importance = \"permutation\" # Include the importance so we can use vi later\n",
    "    )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6fd041-85ba-446e-b12b-d9ca166493ec",
   "metadata": {},
   "source": [
    "Bei der Wahl der Engine können wir in `set_engine()` dem Argument `importance` den Wert `\"permutation\"` übergeben. Das bedeutet, dass während des Trainings des Modell die Feature Importance berechnet wird. Damit können wir später mithilfe von `vi` berechnen, welche Prädiktoren den größten Einfluss auf die Zielvariable haben. Prinzipiell ist das ein optionaler Schritt, den wir nur brauchen, wenn wir später die Variable Importance berechnen wollen.\n",
    "\n",
    "`rand_forest()` hat neben der Engine und dem Modus 3 einstellbare Parameter. Aus der [Dokumentation](https://parsnip.tidymodels.org/reference/rand_forest.html) können wir entnehmen:\n",
    "\n",
    "* `mtry`: An integer for the number of predictors that will be randomly sampled at each split when creating the tree models. Default value is the square root of the number of predictors.\n",
    "* `trees`: An integer for the number of trees contained in the ensemble. Default value is 500.\n",
    "* `min_n`: An integer for the minimum number of data points in a node that are required for the node to be split further. Default value is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70ccfd-a6ff-4f99-84a3-9bfcb0d4fe8a",
   "metadata": {},
   "source": [
    "### `recipe`\n",
    "\n",
    "Die Vorbereitung via `recipe` bleibt ebenfalls gleich. Da Random Forests auf einer Vielzahl von Decision Trees beruhen, sind die Grundannahmen beim Pre-Processing die selben. Auch Random Forests sind invariant gegenüber Standardization. Wir können also einfach das Rezept aus dem vorherigen Beispiel übernehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f525da-6ffc-410b-98db-e0af3613d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_recipe <- train_data %>%\n",
    "    recipe(cnt ~ .) %>%\n",
    "    step_mutate(across(where(is.logical), as.factor)) %>% # Mutate logical variables to factors\n",
    "    step_dummy(all_nominal_predictors()) %>%              # Encode categorical variables if needed\n",
    "    step_impute_mean(all_numeric_predictors()) %>%        # Impute missing values for numeric predictors\n",
    "    step_corr(all_predictors()) %>%                       # Remove correlating predictors\n",
    "    step_zv(all_predictors()) %>%                         # Remove zero-variance predictors\n",
    "    prep()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d17273-47e4-44d9-af18-82a9ddc14a9d",
   "metadata": {},
   "source": [
    "### `workflows`\n",
    "\n",
    "Nachdem wir Modell und Rezept fertig definiert haben, können wir wieder einen Workflow erstellen und diesen auf den Trainingsdaten trainieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db904b53-8c35-4311-89a8-c29a62eb1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_workflow <- workflow() %>%\n",
    "    add_model(bikes_model) %>%\n",
    "    add_recipe(bikes_recipe)\n",
    "\n",
    "bikes_fit <- bikes_workflow %>% \n",
    "    fit(data = train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae18d1a-e78d-4912-93d9-15ba8cacbf7c",
   "metadata": {},
   "source": [
    "## Modell evaluieren\n",
    "\n",
    "Abschließend können wir das Modell noch evaluieren..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84995e3-27fe-4bd3-b673-3bcc6f2260d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_fit %>% \n",
    "    augment(test_data) %>% \n",
    "    metrics(truth = cnt, estimate = .pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a12e3-5b8a-4985-abb1-b0a11feb82cd",
   "metadata": {},
   "source": [
    "...und die Variable Importance berechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a8188-6bad-4d3d-8a59-17e3e64b01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_fit %>% \n",
    "    extract_fit_parsnip() %>% # extract the model\n",
    "    vi(scale = TRUE) %>%      # scale the most important variable to 100\n",
    "    head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df13461-9803-4915-af08-e94627fb6d7d",
   "metadata": {},
   "source": [
    "Vergleicht die Metriken und die Variable Importance mit dem einfach Decision Tree-Modell. An welchen Stellen hat sich unsere Modellierung verbessert? Was ist jetzt anders?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e35922-5c61-4c6c-bc28-9eaa260a9098",
   "metadata": {},
   "source": [
    "## Modell tunen\n",
    "\n",
    "<img src=\"https://tune.tidymodels.org/logo.png\" alt=\"rsample\" width=\"100\" align=\"right\" /> Ein weiterer Schritt zur Modellverbesserung im Machine Learning ist das sogenannte Tuning. Beim Tuning probieren wir verschiedene Modellparameter und ihren Effekt auf Vorhersagefähigkeit und Performance des Modells aus.\n",
    "\n",
    "Wir erinnern uns, dass die Random Forest-Implementierung in `tidymodels` drei Hyperparameter hatte: `mtry`, `trees` und `min_n`. Diese drei Parameter sind unsere Stellschrauben für das Tuning. Wir können sie manuell verändern und nach jeder Veränderung das Modell trainieren und evaluieren und anschließen vergleichen, welchen Einfluss die Änderung der Hyperparameter auf Vorhersagefähigkeit hat.\n",
    "\n",
    "Dafür gibt es in `tidymodels` das Paket `tune`. Mit `tune` können wir den Tuning-Prozess komfortabel und automatisiert ausführen. Dazu müssen wir zunächst ein eigenes Modell für das Tuning definieren. In dieser Modelldefinition definieren wir die Hyperparameter nicht manuell, sondern spezifizieren mit der Funktion `tune()`, dass sie später getuned werden sollen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3be0d4-0de9-4049-98d7-d33b859bed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_model <- rand_forest(\n",
    "    mode = \"regression\",            # Build a regression model\n",
    "    mtry = tune(),\n",
    "    trees = tune(),\n",
    "    min_n = tune()\n",
    "    ) %>%\n",
    "    set_engine(\n",
    "        \"ranger\",                  # Using the `ranger` engine\n",
    "        importance = \"permutation\" # Include the importance so we can use vi later\n",
    "    )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42de21-5399-4fd9-bd06-853cc6595ba8",
   "metadata": {},
   "source": [
    "Dann müssen wir mit dem neuen Modell einen eigenen Workflow definieren - dazu gleich mehr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f7173-910e-45b9-8c26-ed5c6f77debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_workflow <- workflow() %>%\n",
    "    add_model(tune_model) %>%\n",
    "    add_recipe(bikes_recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fb545-c32c-44ff-b085-864c44264f09",
   "metadata": {},
   "source": [
    "### V-fold cross-validation\n",
    "\n",
    "Im Machine Learning wird für das Tuning von Hyperparametern oft eine sogenannte **V-fold** (oder k-fold) **cross-validation** genutzt. Bei diesem Prozess werden die Trainingsdaten in $V$ gleich große Sets aufgeteilt. Bei jedem Validierungsdurchlauf werden $V-1$ Sets als Trainings-Sets benutzt, und ein Set als **Validation Set**.\n",
    "\n",
    "V-fold cross-validation hilft uns dabei, Overfitting auf bestimmte Hyperparameter zu vermeiden. Außerdem erhalten wir eine robustere Aussage über die Vorhersagefähigkeit und die Performance des Modells. Ebenso ist V-fold cross-validation bei kleinen Datensätzen hilfreich, weil wir durch das Durchtauschen die einzelnen Datenpunkte sowohl für Training als auch Validierung nutzen. \n",
    "\n",
    "Aber aufgepasst: ein Validierungsset ersetzt nicht das Test-Set! Dieses sollten wir immer vorher beiseite legen, um eine von den Trainingsdaten unabhängige Evaluierung zu ermöglichen.\n",
    "\n",
    "In `tidymodels` können wir Validation Sets mithilfe der Funktion `vfold_cv` erstellen. Als typischer Wert bietet sich bspw. $V = 5$ an: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04666f-834d-4e05-9ba4-393ce480ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds <- vfold_cv(train_data, v = 5)  # V = 5\n",
    "cv_folds %>% glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b20eb1-4d8d-42cc-9be7-9252bbe6c56b",
   "metadata": {},
   "source": [
    "### Tuning Grid\n",
    "\n",
    "Das Tuning findet dann auf einem sogenannten Grid statt. Bei drei Hyperparametern können wir uns das Tuning im Prinzip wie ein drei-dimensionales Optimierungsproblem vorstellen. Optimieren bzw. minimieren wollen wir dabei den Root Mean Square Error `rmse`, der eine Aussage darüber trifft, wie gut unsere Modellvorhersage ist. \n",
    "\n",
    "`tune` hat verschiene Grid-Funktionen, wir benutzen zunächst einmal `grid_random()`. Für jeden Hyperparameter (also `mtry`, `trees` und `min_n`) können wir eine Spanne einstellen, über die der Hyperparameter getuned werden soll:\n",
    "\n",
    "* `mtry`: Dieser Parameter beschreibt die Anzahl der zufällig ausgewählten Features bei jedem Split eines Trees. Ein gut funktionierender Wert ist i. d. R. $\\sqrt{n}$, wobei $n$ die Anzahl an Prädiktoren ist. Mögliche Werte für `mtry` liegen zwischen 1 und der Anzahl an Prädiktoren.\n",
    "* `trees`: Dieser Parameter definiert die Anzahl an einzelnen Decision Trees, die das Modell trainiert. Ein Random Forest mit einer niedrigen Anzahl an Trees lässt sich schneller trainieren, wird aber keine so genauen Aussagen treffen können. Ab einer bestimmten Anzahl an Trees wird die Vorhersagefähigkeit des Modells jedoch auch nicht mehr besser, und die *computational costs* für das Training des Modells werden zu groß.\n",
    "* `min_n`: Dieser Parameter beschreibt die minimale Anzahl an Datenpunkte innerhalb eines Splits. Eine sehr niedrige Zahl führt i. d. R. zu Overfitting, kann aber dabei helfen, komplexe und kleine Cluster innerhalb der Daten zu entdecken. Eine größere Zahl bewirkt das Gegenteil.\n",
    "\n",
    "Mit `size` bestimmen wir anschließend die Anzahl an verschiedenen, zufälligen Kombinationen, die unser Tuning Grid annehmen soll. Hier lohnt es sich, erst einmal mit einem kleinen Wert zu starten, und sich anschließend auf die Bereiche im Grid zu konzentrieren, die die höchste Vorhersagefähigkeit erzeugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd7d4a-9b82-4e7f-918c-d6089afd9f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_grid <- grid_random(\n",
    "    mtry(range = c(1, ncol(train_data) - 1)),      \n",
    "    trees(range = c(200, 1000)), \n",
    "    min_n(range = c(2, 40)) ,    \n",
    "    size = 6               \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614aa852-f03c-4d1a-be3f-f48b9e60a0d4",
   "metadata": {},
   "source": [
    "Das eigentliche Tuning wird dann mithilfe der Funktion `tune_grid()` ausgeführt.\n",
    "\n",
    "An dieser Stelle wird die Sinnhaftigkeit von `workflows` deutlich - das Modell muss ja jedes Mal die Trainingsdaten pre-processen und fitten. Unser Code wird viel übersichtlicher, weil wir diesen Prozess in `tune_workflow` gespeichert haben. Als Trainingsdaten nutzen wir die cross-validation sets, die wir vorher erstellt haben. Für jede Parameter-Kombination auf unserem Tuning Grid werden die Metriken `rmse` und `rsq` berechnet.\n",
    "\n",
    "Das Tuning kann je nach Serverauslastung 5-10 Minuten dauern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26130260-5f99-4f19-b827-cc677819998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_results <- tune_grid(\n",
    "    tune_workflow,\n",
    "    resamples = cv_folds,\n",
    "    grid = tune_grid,\n",
    "    metrics = metric_set(rmse, rsq)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede9990-b369-4261-bc63-e6114eed9b6e",
   "metadata": {},
   "source": [
    "### Tuning evaluieren & visualisieren\n",
    "\n",
    "Jetzt müssen wir das Tuning evaluieren. Dafür können wir die Metriken aus den Tuning-Ergebnissen mithilfe von `collect_metrics()` rausziehen. Wir erhalten eine saubere Tabelle, in der die verschiedenen Parameter-Kombinationen des Tuning Grids und ihre dazugehörigen Metriken abgebildet sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5dde94-9083-4481-9c4c-e010e9faab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_results %>%\n",
    "    collect_metrics() %>%\n",
    "    select(mtry, trees, min_n, .metric, mean) %>%\n",
    "    pivot_wider(\n",
    "        names_from = .metric,\n",
    "        values_from = mean\n",
    "        ) %>%\n",
    "    arrange(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe780a-f279-4c2d-a0cb-8364a610fb0a",
   "metadata": {},
   "source": [
    "Je niedriger der Wert für `rmse`, umso besser ist die Vorhersagefähigkeit des Modells. Wie wir sehen können, hat das Tuning das Modell auch nochmal etwas verbessert, im Vergleich zum einfachen Modellieren ohne Tuning zu Beginn des Notebooks.\n",
    "\n",
    "In einem echten Anwendungsfall würden wir jetzt das Tuning Grid unserer Hyperparameter noch weiter erkunden, um die bestmögliche Kombination von `mtry`, `trees` und `min_n` zu finden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5595723f-d776-40c0-9004-328b71fe743d",
   "metadata": {},
   "source": [
    "### Bestes Modell auswählen...\n",
    "\n",
    "Aus Zeitgründen wählen wir aber bereits an dieser Stelle das beste Modell aus, und zwar mithilfe von `select_best()`. Mit `finalize_workflow()` definieren wir den finalen Workflow und somit auch das finale Modell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b4139-f2ab-4620-a1e7-3796ee52ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf <- tune_results %>%\n",
    "    select_best(metric = \"rmse\")\n",
    "\n",
    "final_rf <- tune_workflow %>%\n",
    "    finalize_workflow(best_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade5fba-3c0b-4002-ab50-e700da66fb48",
   "metadata": {},
   "source": [
    "### ...und fitten\n",
    "\n",
    "...und fitten das ganze wie gewohnt auf den Trainingsdaten.\n",
    "\n",
    "Anschließend können wir das finale Modell noch mithilfe der Testdaten evaluieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35107ea2-7994-4161-a673-d62ed89316e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fit <- final_rf %>% fit(data = train_data)\n",
    "\n",
    "final_fit %>% \n",
    "    augment(test_data) %>% \n",
    "    metrics(truth = cnt, estimate = .pred)\n",
    "\n",
    "final_fit %>% \n",
    "    extract_fit_parsnip() %>% \n",
    "    vi(scale = TRUE) %>%      \n",
    "    head(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
