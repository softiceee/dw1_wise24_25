{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566bc926-fef9-4e85-a193-1c132ce84ec1",
   "metadata": {},
   "source": [
    "# k-means Clustering\n",
    "\n",
    "In diesem Notebook geht es um Clustering von unstrukturierten Daten mit dem **k-means**-Algorithmus. k-means ist ein Klassifizierungs-Algorithmus im Bereich des *unsupervised learning* (unüberwachtes Lernen). Dadurch unterscheidet er sich von den bisher kennengelernten Modellen, die alle auf Supervised Learning beruhen. k-means versucht bei einer großen Menge unstrukturierter Daten die einzelnen Datenpunkte in **Cluster** (Gruppen) aufzuteilen. Ähnliche Datenpunkte werden dabei demselben Cluster zugeordnet. Clustering-Algorithmen können Strukturen und Muster in unbekannten (ungelabelten) Datensätzen erkennen.\n",
    "\n",
    "Die Idee zu k-means stammt ursprünglich aus den 1950er/1960ern und ist damit einer der ältesten ML-Algorithmen. k-means ist ein sehr einfacher und schneller Algorithmus und kann dadurch effizient mit sehr großen unstrukturierten Datenmengen umgehen. Typische Anwendungen von Clustering-Algorithmen sind bspw. Kund:innengruppen im digitalen Marketing, Netzwerkanalysen auf Social Media oder Personalized Recommendation Systems.\n",
    "\n",
    "In diesem Notebook werden wir eine einfache Implementierung von k-means kennenlernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25917a0-a46e-4785-9f0e-63f85b0f1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacman::p_load(tidyverse, tidymodels, tidyclust)\n",
    "options(repr.plot.width = 12, repr.plot.height = 8) # Jupyter display options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57d12f-082c-4c26-b53e-50150f6bed19",
   "metadata": {},
   "source": [
    "## Wie funktioniert k-means Clustering?\n",
    "\n",
    "Die mathematischen Grundlagen für k-means werden in der Vorlesung hergeleitet. Der Algorithmus lässt sich aber relativ einfach visualisieren und wird dadurch anschaulich nachvollziehbar.\n",
    "\n",
    "Artwork by @allisonhorst\n",
    "\n",
    "<a title=\"Artwork by @allisonhorst\" href=\"https://www.tidymodels.org/learn/statistics/k-means/kmeans.gif\"><img width=\"50%\" alt=\"K-means convergence illustrated by @allisonhorst\" src=\"https://www.tidymodels.org/learn/statistics/k-means/kmeans.gif\"></a>\n",
    "\n",
    "<a href=\"https://commons.wikimedia.org/wiki/File:K-means_convergence.gif\">Chire</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">CC BY-SA 4.0</a>, via Wikimedia Commons\n",
    "\n",
    "<a title=\"Chire, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:K-means_convergence.gif\"><img width=\"30%\" alt=\"K-means convergence\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/512px-K-means_convergence.gif\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30d5a37-9b21-44b0-bd01-80162dd5d351",
   "metadata": {},
   "source": [
    "## Zufällige Datenpunkte generieren"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59c43a42-e881-4b84-941a-55f89d6ba926",
   "metadata": {},
   "source": [
    "Wir generieren zunächst zweidimensionale Zufallsdaten, bestehend aus drei Clustern. Die einzelnen Datenpunkte in jedem Cluster folgen einer multivariaten Gauß-Verteilung. \n",
    "\n",
    "Wir merken uns die Label der Cluster, werden diese aber nicht im Training verwenden! Der Algorithmus kennt also nur die zufällig generierten Datenpunkte **ohne** Label und versucht innerhalb dieser ein Muster zu erkennen!\n",
    "\n",
    "> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Multivariate_normal_density.png/450px-Multivariate_normal_density.png\"  alt=\"zweidimensionale Gauss-Verteilung\" width=\"150\" align=\"right\" > Eine multivariate Gauß-Verteilung (\"Normalverteilung\") ist eine Gauß-Verteilung in mehreren Dimensionen. Vielleicht hilft euch das Bild rechts (Bildquelle: <a href=\"https://de.wikipedia.org/wiki/Mehrdimensionale_Normalverteilung\">Wikipedia</a>), damit ihr euch eine Gauß-Verteilung für zwei Variablen vorstellen könnt. Die Gauß-Verteilung selbst wird in der Vorlesung ausführlich erklärt. \n",
    "\n",
    "Um die Datenpunkte zu generieren, erzeugen wir zunächst einen `tibble` namens `centers` aus 3 Reihen. Jede Reihe enthält die \"Metadaten\" eines Clusters, nämlich das Label `cluster`, die Anzahl der Punkte `num_points` sowie die Koordinaten `x1` und `x2` für die beiden Dimensionen. Mit `glimpse()` können wir uns den `tibble` wie gewohnt ausgeben lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e729b7-4d30-4633-9d8a-485696de55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers <- tibble(\n",
    "    cluster = factor(1:3),         \n",
    "    num_points = c(100, 150, 75),  # number points in each cluster\n",
    "    x1 = c(3, 0, -2),              # x1 coordinate of cluster center\n",
    "    x2 = c(-1, 1, -2)              # x2 coordinate of cluster center\n",
    "    ) %>%\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93be824-73ac-4d59-8331-eaea8b37af71",
   "metadata": {},
   "source": [
    "Anschließend müssen wir die Datenpunkte für jeden der Center-Punkte generieren! \n",
    "\n",
    "Für die beiden Dimensionen `x1` und `x2` wird mithilfe von `mutate()` und `map2()` eine Liste an zufällig erzeugten Datenpunkten generiert. `rnorm` erzeugt für uns dabei zufällige Werte, die einer Gauß-Verteilung folgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5195f-d2ff-4301-9515-536ee218595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_distributions <- centers %>%\n",
    "    mutate(\n",
    "        x1 = map2(num_points, x1, rnorm), # generate data points in dimension x1\n",
    "        x2 = map2(num_points, x2, rnorm)  # generate data points in dimension x2\n",
    "    ) %>% \n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6be264-6cf4-49a4-818c-adbb3c5f1968",
   "metadata": {},
   "source": [
    "Anschließend können wir mit `unnest()` die Liste **entpacken**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693373e-5b7d-43ae-b1d0-d40ce963ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_labeled <- centers_distributions %>%\n",
    "    select(-num_points) %>% \n",
    "    unnest(cols = c(x1, x2)) %>% # unnest the data points in both dimensions\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde77f4-1169-497a-b53d-fb8f8b296729",
   "metadata": {},
   "source": [
    "... und mit `ggplot2` visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c40c07e-bb04-4264-a8f2-0feea7974901",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_labeled %>%\n",
    "    ggplot(aes(x1, x2, color = cluster)) +\n",
    "    geom_point(size = 3) +\n",
    "    geom_point(data = centers, aes(x = x1, y = x2), shape = \"X\", size = 15) +\n",
    "    theme_minimal(base_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a688b8f-8206-45ef-9d63-17abdb04e21b",
   "metadata": {},
   "source": [
    "Soweit so gut! Wir haben jetzt drei Cluster erzeugt und schon rein optisch lassen sich diese sehr gut trennen, wobei es auch einzelne Datenpunkte gibt, die räumlich recht weit von ihrem Cluster entfernt oder sich mit anderen Clustern überlappen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014cf07-9c93-4360-84b6-92c2c8c7d50d",
   "metadata": {},
   "source": [
    "Um k-means zu testen, müssen wir allerdings die Information über die Cluster entfernen! k-means soll ja nicht wissen, welcher Punkt zu welchem Cluster gehört.\n",
    "\n",
    "Das machen wir wie gewohnt mit `select()`. Mit `sample_n()` durchmischen wir anschließend die Datenpunkte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfceacba-c73b-470a-b438-82e0e58ae070",
   "metadata": {},
   "outputs": [],
   "source": [
    "points <- points_labeled %>% \n",
    "    select(-cluster) %>%\n",
    "    sample_n(nrow(.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae104aa9-ee3e-40b2-af75-0593e92e6d75",
   "metadata": {},
   "source": [
    "Ohne die Farbinformationen für die verschiedenen Cluster sieht unser Datensatz jetzt folgendermaßen aus. Es ist direkt schwieriger, Clusterstrukturen zu erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84969ad1-feb1-40f8-8b05-2e00f1c347cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "points %>%\n",
    "    ggplot(aes(x1, x2)) +\n",
    "    geom_point(size = 5) +\n",
    "    theme_minimal(base_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa4470-df0c-4193-8863-399bf4fab629",
   "metadata": {},
   "source": [
    "## Das Modell definieren\n",
    "\n",
    "Wie bereits bekannt, definieren wir zunächst ein einfaches k-means-Modell mithilfe von `tidymodels`. k-means ist über die Funktion `k_means()` hinterlegt.\n",
    "\n",
    "Mit `set_engine()` können wir die Parameter des Modells definieren. \n",
    "\n",
    "Wir wählen eine einfache k-means-Implementierung von `stats`. Der einzige Parameter, der für k-means zwingend notwendig ist, ist die Anzahl der **centers**, von denen der Algorithmus ausgehen soll. Diese Anzahl ist bekannt als **k-value** - daher hat der Algorithmus auch seinen Namen. Wir setzen $k$ mithilfe von `num_clusters`.\n",
    "\n",
    "In der Realität kennen wir $k$ nicht von Anfang an, sondern müssen raten. Wir wir später sehen werden, gibt es verschiedene Methoden, um ein geeignetes $k$ zu finden. Wir schummeln an dieser Stelle der Einfachheit halber und starten mit $k=3$. Weitere Modellparameter lernen wir später kennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cf8ab-94ac-4ac5-9ac4-640c1b1461f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- k_means(num_clusters = 3) %>%\n",
    "    set_engine(\"stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168aedd-cf96-4052-b604-16cc2b87c969",
   "metadata": {},
   "source": [
    "Wie gewohnt können wir das Modell mithilfe von `fit()` trainieren. Als Datensatz übergeben wir den ungelabelten Datensatz `points`. Der Algorithmus wählt selbstständig die Startpunkte und versucht dann, zu konvergieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ee2a1-c7de-4a7f-94c0-7fa00af07f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model <- model %>%\n",
    "    fit(~ ., data = points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de82d62-56a6-4be0-9f12-b0b73662e74c",
   "metadata": {},
   "source": [
    "## `broom`\n",
    "\n",
    "<img src=\"https://broom.tidymodels.org/logo.png\" alt=\"broom\" width=\"100\" align=\"right\" /> Mithilfe des Pakets `broom` von `tidymodels` können wir das trainierte Modell anschließend inspizieren. \n",
    "\n",
    "Um die Modellinformationen abzurufen, können wir die Funktion `extract_fit_engine()` auf das trainierte Modell `fit_model` anwenden. \n",
    "\n",
    "Als erste Funktion von `broom` können wir uns `augment()` anschauen, was wir bereits kennengelernt haben. Mit `augment()` können wir die vorhergesagten neuen Labels direkt mit den ungelabelten Trainingsdaten verbinden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8aff5-85b7-4f1f-b546-28350a764a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model %>%           \n",
    "    augment(points) %>% \n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c8fd0a-1fcc-44b3-b3bf-4f16a678cf1e",
   "metadata": {},
   "source": [
    "Wie wir sehen, wird jeder Datenpunkt einem Cluster zugeordnet.\n",
    "\n",
    "Mit `tidy()` können wir Informationen auf der Cluster-Ebene ausgeben, nämlich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfffa72-b64c-4fb1-bad3-4fa63855a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model %>% \n",
    "    extract_fit_engine() %>%\n",
    "    tidy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e4389-c5c5-4b24-bd36-78429e94ad75",
   "metadata": {},
   "source": [
    "**Frage:**\n",
    "1. Welche Informationen gibt `tidy()` aus? Benutzt eine Suchmaschine oder generative KI, und diskutiert die einzelnen Variablen zu zweit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf52d2d-07f0-4d83-8144-1f9463145e40",
   "metadata": {},
   "source": [
    "Mit `glance()` können wir Informationen auf der Modell-Ebene ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3c2cb-d4ce-4329-aa6a-6d9c2cde5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model %>% \n",
    "    extract_fit_engine() %>%\n",
    "    glance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff9cd8-8cbc-415e-b32f-564bda66035e",
   "metadata": {},
   "source": [
    "**Fragen:**\n",
    "\n",
    "2. Welche Informationen gibt `glance()` aus? Recherchiert wieder die Bedeutung der einzelnen Variablen. Was beschreibt der Quotient `betweenss` / `totss`? Welche Variablen versucht k-means zu minimieren und welche zu maximieren?\n",
    "\n",
    "3. Wieso braucht der Algorithmus nur so wenige Iterationen, um zu konvergieren?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0f609-92b5-4868-ab87-5d898b9374e4",
   "metadata": {},
   "source": [
    "## Verformelung der Metriken von k-means\n",
    "\n",
    "- $x_i$: data point $i$ in the total set of points $C_{total}$\n",
    "- $C_k$: set of points $C_k$ in cluster $k$\n",
    "- $\\mu_k$: centroid of cluster $k$\n",
    "- $\\mu_{total}$: centroid of the total set of points $C_{total}$\n",
    "\n",
    "$withinss_k = \\sum_{i \\in C_k} (x_i - \\mu_k)^2$\n",
    "\n",
    "$tot.withinss = \\sum_{k=1}^k withinss_k$\n",
    "\n",
    "$totss = \\sum_{i \\in C_{total}} (x_i - \\mu_{total})^2$\n",
    "\n",
    "$betweenss = totss - tot.withinss$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff1af8-ccc1-4a9d-8c66-3f3e3d1b3aa4",
   "metadata": {},
   "source": [
    "## Visualisierung\n",
    "\n",
    "Wir können jetzt das trainierte Modell mit dem ursprünglich erzeugten Datensatz vergleichen, indem wir beides visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85777e43-d915-4669-b09b-97d3269e2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model %>%           \n",
    "    augment(points_labeled) %>%\n",
    "    ggplot(aes(x = x1, y = x2, color = cluster, shape = .pred_cluster)) +\n",
    "    geom_point(size = 5) + \n",
    "    theme_minimal(base_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee17d5e8-9138-4067-9246-588baf5619e2",
   "metadata": {},
   "source": [
    "* **Frage:**\n",
    "\n",
    "4. Wie interpretiert ihr die Grafik? Welche Datenpunkte wurden vom Algorithmus falsch zugeordnet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886092e-1de5-448a-9ada-939d0f0df28f",
   "metadata": {},
   "source": [
    "## Das richtige $k$ herausfinden\n",
    "\n",
    "Die Zusammenfassungen mit `augment()`, `tidy()` und `glance()` werden erst so richtig praktisch, wenn wir sie in Kombinationen mit weiteren Tools von `dplyr` verwenden. Wir können z.B. mal annehmen, dass wir die richtige Anzahl der Cluster $k$ nicht kennen, also eine Analyse für verschiedene Werte von $k$ vornehmen wollen. \n",
    "\n",
    "Das würde eigentlich einem Tuning von Modellparametern entsprechen, allerdings befindet sich `tidyclust` noch so weit in der Entwicklung, dass es noch nicht in `tune` eingebunden werden kann. Wir müssen das Tuning also per Hand vornehmen.\n",
    "\n",
    "Mithilfe von `map()` könnten wir dann sehr schnell über Werte von $k$, z.B. von 1 bis 9, iterieren und unsere Ergebnisse in einem einzigen, großen `tibble` abspeichern. \n",
    "\n",
    "> Die Funktion `map()` ist wie eine \"Stapelverarbeitung\": Sie wendet eine andere Funktion auf eine Reihe von Objekten an und gibt dann die durch die Funktion veränderten Objekte zurück. Im Prinzip funktioniert `map()` wie eine Schleife, hat aber eine bessere Performance. `map()` kann auch komfortabel in der Pipe eingesetzt werden.\n",
    "\n",
    "Wir erzeugen zunächst einen `tibble` mit Werten von $k$ zwischen 1 und 9. Anschließend erzeugen wir mit `mutate()` eine neu Spalte namens `kclust`. In `kclust` wird für jedes $k$ ein eigenes Modell trainiert. Das Modelltraining erfolgt wie gehabt mithilfe von `k_means()` und `fit()`. Wie wir sehen, können wir durch die geschweiften Klammern `{}` einen ganzen Code-Block innerhalb von `mutate()` ausführen. Tendenziell sollte man versuchen, verschachtelten Code zu vermeiden. Allerdings gibt es Anwendungsfälle, in denen es nicht anders möglich ist, wie bei diesem Beispiel.\n",
    "\n",
    "Nachdem wir für jedes $k$ ein eigenes Modell gefittet und in der Spalte `kclust` gespeichert haben, können wir mit `map()` die Befehle `tidy()`, `glance()` und `augment()` auf `kclust` anwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd650168-ea9a-4b8a-b0e7-15050f556a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodels <- tibble(k = 1:9) %>%             # iterate k from 1 to 9\n",
    "\n",
    "    # fit a kmeans model for each k\n",
    "    mutate(\n",
    "        kclust = map(k, ~{                 # map the kmeans model pipeline on k\n",
    "            k_means(num_clusters = .x) %>% # as before define the model\n",
    "            set_engine(\"stats\") %>%        # set the engine\n",
    "            fit(~ ., data = points)        # and fit the data\n",
    "    })) %>%\n",
    "\n",
    "    # apply tidy(), glance() and augment() on each k-instance of kmeans\n",
    "    mutate(\n",
    "        tidied = map(kclust, tidy),                              # map tidy() on kclust\n",
    "        glanced = map(kclust, glance),                           # map glance() on kclust\n",
    "        augmented = map(kclust, ~augment(.x, new_data = points)) # map augment() on kclust – we have to specify the data points\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce1604-dbac-47fe-b43b-540f42684766",
   "metadata": {},
   "source": [
    "Schauen wir uns das Ergebnis mal an – dafür müssen wir die Funktion `glimpse()` benutzen, andernfalls bekommen wir eine Fehlermeldung ausgeworfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf44daf-31df-45e5-bc08-b31a43b488ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodels %>% glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeabb15-1ab9-48f9-9d0a-6daee1bc69a8",
   "metadata": {},
   "source": [
    "Der Datensatz sieht etwas anders aus als sonst. \n",
    "\n",
    "* In der ersten Spalte `k` erkennen wir die Werte für $k$ von 1 bis 9.\n",
    "* Die Spalte `klcust` ist vom Typ `list` und enthält unser Modell. Listen haben den Vorteil, dass sie Objekte von verschiedenen Datentypen beinhalten können. `glimpse()` zeigt daher für Listen immer den Objekttyp an. Jedes kmeans-Modell besteht aus verschiedenen Objekten unterschiedlichen Typs.\n",
    "* Die Spalten `tidied` und `glanced` enthalten Objekte vom Typ `tbl_df` – ihr erinnert euch daran, dass beide Funktionen einen `tibble` mit den verschiedenen Metriken bzw. Informationen über das Modell ausgeben.\n",
    "* `augmented` enthält ebenfalls viele `tbl_df`-Objekte, die alle die Dimension 325x3 haben. Das liegt daran, dass wir 325 Datenpunkte vorliegen haben, und jeder Datenpunkt hat zwei Koordinaten (`x1` und `x2`) sowie einen vorhergesagten Cluster.\n",
    "\n",
    "Um sinnvoll weiterzuarbeiten, speichern wir die einzelnen Listen in eigenen Objekten. Dafür müssen wir sie mit der Funktion `unnest()` entpacken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2bb5ad-2082-417d-978d-2da150ea36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters <- kmodels %>%\n",
    "    select(k, tidied) %>%\n",
    "    unnest(tidied)\n",
    "\n",
    "clusterings <- kmodels %>%\n",
    "    select(k, glanced) %>%\n",
    "    unnest(glanced)\n",
    "\n",
    "assignments <- kmodels %>%\n",
    "    select(k, augmented) %>%\n",
    "    unnest(augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699ec81-5d21-4bc6-8c42-95d385d21ab5",
   "metadata": {},
   "source": [
    "Nachdem wir die Listen entpackt haben, sehen die `tibbles` auch schon wieder bekannter aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863152b-8b6f-44fd-b7c7-a7f0f989ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments %>% head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f995c-3315-490e-ba56-709328370e77",
   "metadata": {},
   "source": [
    "Durch unser bisheriges Vorgehen ist es jetzt ein leichtes, die Modelle für die verschiedenen Werte von $k$ zu vergleichen.\n",
    "\n",
    "Mit einer einfachen Pipe können wir die verschiedenen Modelle für $k=1$ bis $k=9$ in einem einzigen Diagramm visualisieren. Wir erhalten relativ schnell einen guten Überblick über die verschiedenen Werte von $k$ und können das Diagramm bereits zur ersten Einschätzung eines geeigneten Wertes von $k$ nutzen.\n",
    "\n",
    "Dafür benutzen wir `facet_wrap()` und iterieren die Sub-Plots über `k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892d7e0-6bbb-4aa7-aca2-562ffd42f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments %>%\n",
    "    ggplot(aes(x = x1, y = x2)) +\n",
    "    geom_point(aes(color = .pred_cluster), alpha = 0.8) + \n",
    "    facet_wrap(~ k) +\n",
    "    theme_minimal(base_size = 15) +\n",
    "    geom_point(data = clusters, size = 10, shape = \"x\") # add data from tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd28df5-8534-4ebb-b3ea-3c761b8b1176",
   "metadata": {},
   "source": [
    "## Elbow-Method\n",
    "\n",
    "Im Normalfall ist es nicht trivial, eine geeignete an Clustern zu bestimmen. Größere Datensätze haben i. d. R. nicht nur zwei Features, und lassen sich dadurch schwierig visualisieren.\n",
    "\n",
    "K-means kennt einige Verfahren zur Bestimmung von $k$, die mathematisch mehr oder weniger fundiert sind. Dazu gehören:\n",
    "\n",
    "1. Elbow method\n",
    "2. Silhouette method\n",
    "3. Gap statistic\n",
    "\n",
    "Diese werden detaillierter z.B. in diesem [Artikel](https://uc-r.github.io/kmeans_clustering) erklärt. \n",
    "\n",
    "Beispielhaft werden wir an dieser Stelle einmal kurz auf die **Elbow method** eingehen. Diese wird auch in der Vorlesung beschrieben und hergeleitet.\n",
    "\n",
    "Für die Elbow method brauchen wir die Metrik **total within-cluster sum of square** `tot.withinss`. Dieser Wert wird von `glance()` ausgegeben und wir hatten ihn weiter oben im Objekt `clusterings` gespeichert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b763c-e141-4a27-beb8-18b8306abcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba01b3d-e651-4d1e-9970-56a772ae4015",
   "metadata": {},
   "source": [
    "Wenn wir also `tot.withinss` gegen `k` plotten, erhalten wir folgenden Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1bb72-c1b4-4f1a-bf12-4e0642f17b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterings %>%\n",
    "    ggplot(aes(k, tot.withinss)) +\n",
    "    geom_line(linewidth = 1) +\n",
    "    geom_point(size = 5) +\n",
    "    theme_minimal(base_size = 25) +\n",
    "    scale_x_continuous(breaks = seq(1, 9, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb042cc-c780-49dd-99b4-43ff3480f149",
   "metadata": {},
   "source": [
    "**Frage:**\n",
    "\n",
    "5. Bei welchem $k$ seht ihr den Ellbogen?\n",
    "\n",
    "Eine Krümmung (englisch: bend), bzw. ein sogenannter Ellbogen, symbolisiert i.d.R. einen geeigneten Wert von $k$ für eine Clusterlösung. Dieser Ellbogen indiziert, dass das Hinzufügen weiterer Cluster das Modell nicht verbessert. In diesem Beispiel liegt der Ellbogen bei $k=3$, was ja mit unserer vorab generierten Anzahl der Cluster übereinstimmt.\n",
    "\n",
    "Eine mathematische Herleitung dieser Aussage findet ihr z.B. unter\n",
    "\n",
    "> [Tibshirani, R., Walther, G. & Hastie. T (2000). Estimating the Number of Clusters in a Dataset via the Gap Statistic. Journal of the Royal Statistical Society, Series B, 63(2), p. 411-423.](https://hastie.su.domains/Papers/gap.pdf)\n",
    "\n",
    "In der Realität wird K-means vor allem bei großen, unstrukturierten Datenmengen eingesetzt. Es besteht daher eine große Unsicherheit, welches $k$ geeignet ist. In diesem Falle sind mathematische Verfahren wie die Elbow method hilfreich, um $k$ zu bestimmen. Zusätzlich sollte man jedoch auch immer die Interpretierbarkeit der Clusterlösung bedenken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f01769-deb8-422a-b944-f00c54167dab",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "Anders als bei den Decision Trees kann bei k-means eine Standardisierung der Datenpunkte das Ergebnis verbessern. Das würde man mit `recipe` mithilfe der Funktion `step_normalize()` umsetzen. Diese Funktion normalisiert den Datensatz in dem Sinne, dass nach dem Schritte der Mittelwert jeder Variable 0 ist und die Standardabweichung 1. Bei unserem Beispiel führt `step_normalize()` zwar zur eine Verbesserung der Metrik `tot.withinss`, allerdings ist diese Verbesserung nicht unbedingt aussagekräftig, da die Zuordnung in die einzelnen Cluster nicht verbessert wird. Das liegt daran, dass unser Datensatz künstlich generiert wurde und dadurch die Datenpunkte den neuen Clustern einfach zugeordnet werden können. \n",
    "\n",
    "Im folgenden Beispielcode könnt ihr das einmal ausprobieren, indem ihr den Code einmal mit dem Schritt `step_normalize()` ausführt und den Schritt einmal weglasst. Ihr könnt die Zeile einfach mit der Raute `#` auskommentieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4653c2-af89-4eab-b1b3-c7282ab4bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_recipe <- recipe(~ ., data = points) %>%\n",
    "    step_normalize(all_predictors()) %>%\n",
    "    step_impute_mean(all_numeric())\n",
    "\n",
    "model <- k_means(num_clusters = 3) %>%\n",
    "    set_engine(\"stats\")\n",
    "\n",
    "kmeans_workflow <- workflow() %>%\n",
    "  add_recipe(clustering_recipe) %>%\n",
    "  add_model(model)\n",
    "\n",
    "kmeans_fit <- kmeans_workflow %>%\n",
    "  fit(data = points)\n",
    "\n",
    "kmeans_fit %>%\n",
    "    extract_fit_engine() %>%\n",
    "    pluck(\"tot.withinss\")\n",
    "\n",
    "kmeans_fit %>%           \n",
    "    augment(points_labeled) %>%\n",
    "    count(.pred_cluster, cluster) %>%\n",
    "    pivot_wider(names_from = cluster, values_from = n, values_fill = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfff323-3638-44b1-b328-c24f73537391",
   "metadata": {},
   "source": [
    "## palmerpenguins\n",
    "\n",
    "In dieser Woche gibt es keine neuen Datensätze als Beispiele. Wenn ihr hier bereits angekommen seid und noch Zeit habt, könnt ihr versuchen, k-means auf den Datensatz der Pinguine `palmerpenguins` anzuwenden. Ein Versuch wäre es, ob ihr es schafft, mit k-means eine Clusterlösung nach den Spezies der Pinguine umzusetzen.\n",
    "\n",
    "Geht dabei wie bisher vor:\n",
    "- Datensatz laden\n",
    "- Datensatz \"unlabeln\"\n",
    "- Features auswählen (startet am besten mit wenigen Features, z.B. `bill_depth_mm` und `bill_length_mm`\n",
    "- Ein `recipe` erstellen (mit Standardisierung)\n",
    "- Das Modell definieren (schaut euch die Parameter `nstart` und `iter.max` in der Dokumentation von k-means an. Es könnte sich lohnen, diese hochzusetzen, damit das Modell besser funktioniert. Ihr könnt die Parameter über `set_engine()` setzen)\n",
    "- Den Workflow definieren und das Modell fitten\n",
    "- Das Modell auf den ursprünglichen Datensatz anwenden... (mit `augment()`)\n",
    "- und die Ergebnisse mit `ggplot2` visualisieren und evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ad8a1-a271-4845-a97a-b6ec764dc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
